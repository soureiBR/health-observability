# RelatÃ³rio TÃ©cnico - Health Observability: Fluxo da AplicaÃ§Ã£o

## Objetivo da AplicaÃ§Ã£o

O **Health Observability** Ã© um sistema de **alerta enriquecido** para incidentes.
Ele **nÃ£o Ã© uma plataforma de logs** â€” ele complementa as ferramentas de logging existentes
(Logs Teste / OpenSearch) adicionando contexto imediato Ã s notificaÃ§Ãµes de queda de serviÃ§o.

---

## O Que a AplicaÃ§Ã£o FAZ vs O Que Ela NÃƒO FAZ

| FAZ | NÃƒO FAZ |
|-----|---------|
| Captura um trecho dos logs do pod **no momento da queda** | Armazenar logs permanentemente |
| Envia esse trecho como parte do **alerta no Discord/Telegram** | Substituir OpenSearch ou Logs Teste |
| Mostra o status do pod (Running, Pending, Failed) | Coletar ou indexar logs continuamente |
| Mostra o motivo de falha (ex: falta de CPU/memÃ³ria) | Ser uma plataforma de observabilidade |

---

## Fluxo Completo (Passo a Passo)

```
PASSO 1                    PASSO 2                    PASSO 3
Uptime Kuma            Webhook Enricher           Discord / Telegram
(VM externa)           (Pod no Cluster K8s)       (NotificaÃ§Ãµes)

[Monitora URLs]  â”€â”€â–¶  [Recebe alerta de queda]  â”€â”€â–¶  [Envia alerta enriquecido]
                            â”‚
                            â”‚  PASSO 2.1: Consulta K8s API
                            â”‚  - Qual pod estÃ¡ associado?
                            â”‚  - O pod estÃ¡ Running/Pending/Failed?
                            â”‚  - Pega as Ãºltimas 50 linhas de log
                            â”‚  - Pega o describe do pod (se Pending/Failed)
                            â”‚
                            â–¼
                       [Monta mensagem com contexto]
```

### Detalhamento de Cada Passo

**PASSO 1 - Uptime Kuma detecta queda:**
- O Kuma faz health check periÃ³dico nas URLs dos serviÃ§os (ex: `https://api.soureicdn.com/debug/healthz`)
- Quando um serviÃ§o retorna erro (500, 503, timeout), o Kuma dispara um webhook HTTP POST

**PASSO 2 - Webhook Enricher recebe o alerta e enriquece:**
- Recebe o nome do monitor e a mensagem de erro
- Consulta a API do Kubernetes para buscar informaÃ§Ãµes do pod relacionado:
  - **Pod Running**: captura as Ãºltimas 50 linhas de log (mÃ¡x 1500 caracteres)
  - **Pod Pending**: executa `kubectl describe` para mostrar por que o pod nÃ£o subiu (ex: falta de recursos)
  - **Pod Failed**: captura tanto os logs quanto o describe
- Extrai o cÃ³digo HTTP do erro (500, 503, etc.)

**PASSO 3 - Envia notificaÃ§Ã£o enriquecida:**
- Formata a mensagem com todos os dados coletados
- Envia para o **Discord** (embed formatado) e/ou **Telegram** (mensagem markdown)
- A equipe recebe no canal a notificaÃ§Ã£o com contexto suficiente para o primeiro diagnÃ³stico

---

## Exemplo Real de NotificaÃ§Ã£o Gerada

Quando o serviÃ§o `gtm-api` cai com erro 503, a equipe recebe no Discord:

```
âŒ gtm-api CAIU

ğŸ“Š Status HTTP: 503 - Service Unavailable
ğŸ”— URL: https://api.soureicdn.com/debug/healthz
ğŸ³ Pod Status: Running
âœ“ Pronto: âŒ NÃ£o
ğŸ“¦ Pod Name: gtm-api-5f8d9a7c-2b3e1

ğŸ“ Ãšltimos Logs:
  2025-01-15T10:32:01Z ERROR: Connection refused to database
  2025-01-15T10:32:02Z ERROR: Health check failed
  2025-01-15T10:32:03Z WARN: Retrying connection...
```

---

## De Onde VÃªm os Logs e Para Onde VÃ£o

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                 â”‚
â”‚   ORIGEM DOS LOGS          DESTINO DOS LOGS                     â”‚
â”‚                                                                 â”‚
â”‚   Kubernetes API  â”€â”€â”€â”€â”€â”€â–¶  Discord / Telegram (notificaÃ§Ã£o)     â”‚
â”‚   (pod stdout/stderr)      NÃƒO sÃ£o armazenados em nenhum DB    â”‚
â”‚                                                                 â”‚
â”‚   SÃ£o as mesmas linhas que aparecem ao executar:                 â”‚
â”‚   $ kubectl logs <pod-name> -n analytics --tail=50              â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Importante:** Os logs capturados sÃ£o **apenas um trecho** (Ãºltimas 50 linhas) e servem
exclusivamente para dar contexto imediato no alerta. Eles **nÃ£o substituem** a consulta
completa no OpenSearch ou Logs Teste para investigaÃ§Ãµes aprofundadas.

---

## RelaÃ§Ã£o com as Ferramentas de Logging Existentes

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     Logs dos Containers       â”‚
                    â”‚     (stdout / stderr)         â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚             â”‚                  â”‚
                 â–¼             â–¼                  â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  OpenSearch â”‚ â”‚  Logs    â”‚ â”‚  Webhook Enricher   â”‚
          â”‚  (completo) â”‚ â”‚  Teste   â”‚ â”‚  (trecho no alerta) â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚              â”‚                  â”‚
               â–¼              â–¼                  â–¼
          InvestigaÃ§Ã£o   InvestigaÃ§Ã£o     Primeiro diagnÃ³stico
          aprofundada    aprofundada      rÃ¡pido no Discord
          histÃ³rico      histÃ³rico        (Ãºltimas 50 linhas)
```

**O Webhook Enricher complementa, nÃ£o compete.** Ele fornece um "preview" rÃ¡pido dos logs
no momento da queda para que a equipe saiba imediatamente o que estÃ¡ acontecendo,
sem precisar abrir o OpenSearch. Para anÃ¡lise completa, as ferramentas de logging
existentes continuam sendo a referÃªncia.

---

## Componentes da AplicaÃ§Ã£o

| Componente | LocalizaÃ§Ã£o | FunÃ§Ã£o |
|---|---|---|
| `sync_kuma.py` | CronJob no Cluster K8s (a cada 6h) | Sincroniza deployments K8s com monitores no Uptime Kuma |
| `kuma_webhook_enricher.py` | Pod no Cluster K8s (namespace analytics) | Servidor Flask que recebe alertas e enriquece com dados do K8s |
| Uptime Kuma | VM externa | Dashboard de monitoramento e disparo de alertas |
| Discord / Telegram | ServiÃ§os externos | Canais de notificaÃ§Ã£o da equipe |

---

## Resumo Executivo

O Health Observability **acelera o tempo de resposta a incidentes**. Quando um serviÃ§o cai,
ao invÃ©s de a equipe receber apenas "serviÃ§o X estÃ¡ fora", ela recebe uma notificaÃ§Ã£o
com o cÃ³digo HTTP, status do pod e um trecho dos logs â€” tudo no Discord/Telegram,
em segundos. Para a investigaÃ§Ã£o completa, a equipe continua usando o OpenSearch / Logs Teste
normalmente.
